What is the key principle of unbundling databases?;The principle involves breaking down traditional database functionalities into specialized components that can be combined in different ways, similar to how Unix tools can be composed using pipes. This allows different storage and processing systems to be connected through dataflow.;<i>(Ref: Unbundling Databases, p. 499-500)</i>
What are the two main approaches to unifying different storage and processing systems?;<p>Two main approaches:</p><ul><li>Federated databases: Providing unified query interface to various storage engines (read integration)</li><li>Unbundled databases: Using event logs and change data capture to synchronize writes across systems</li></ul>;<i>(Ref: Unbundling Databases, p. 501)</i>
What is the difference between timeliness and integrity in data systems?;<p>Two distinct requirements:</p><ul><li>Timeliness: Ensuring users observe up-to-date state (can be eventually consistent)</li><li>Integrity: Absence of corruption and ensuring derived data is correctly maintained (perpetual consistency)</li></ul>;<i>(Ref: Timeliness and Integrity, p. 524)</i>
What is the end-to-end argument in system design?;<p>The end-to-end argument states that certain functions (like duplicate suppression or data integrity) can only be implemented correctly by the application at the endpoints of a communication system. Lower-level systems may provide partial implementations as optimizations, but cannot guarantee complete correctness.</p>;<i>(Ref: The End-to-End Argument for Databases, p. 519)</i>
How can stream processors handle uniqueness constraints without distributed transactions?;<p>Process:</p><ul><li>Route all requests for a value to the same partition</li><li>Process requests sequentially in each partition</li><li>Use local state to track taken values</li><li>Emit success/rejection messages to output stream</li></ul>;<i>(Ref: Uniqueness in log-based messaging, p. 522)</i>
What are the ethical concerns around predictive analytics?;<p>Key concerns include:</p><ul><li>Systematic exclusion of individuals based on predictions</li><li>Amplification of existing biases</li><li>Lack of transparency in decision-making</li><li>Difficulty in appealing automated decisions</li><li>Risk of discrimination through correlated attributes</li></ul>;<i>(Ref: Predictive Analytics, p. 533-534)</i>
What is the "database inside-out" approach?;<p>It involves:</p><ul><li>Treating logs of immutable events as the primary storage</li><li>Deriving other data representations through transformations</li><li>Using stream processing to maintain derived views</li><li>Treating different storage systems as materialized views</li></ul>;<i>(Ref: Designing Applications Around Dataflow, Application code as a derivation function, p. 504-505)</i>
What are the privacy implications of large-scale data collection?;<p>Key implications:</p><ul><li>Creates unprecedented surveillance infrastructure</li><li>Transfers privacy rights from individuals to corporations</li><li>Data can be compromised, sold, or accessed by governments</li><li>Users often cannot meaningfully consent or opt out</li><li>Data becomes a "toxic asset" with risks</li></ul>;<i>(Ref: Privacy and Tracking, p. 536-541)</i>
How can dataflow systems maintain derived state without distributed transactions?;<p>Through:</p><ul><li>Using deterministic event processing</li><li>Maintaining local state in stream processors</li><li>Processing events exactly once</li><li>Using end-to-end request IDs for deduplication</li><li>Making derived data reprocessable</li></ul>;<i>(Ref: Batch and Stream Processing, Maintaining derived state, p. 494-495)</i>
What is the difference between strict and loose constraints in data systems?;<p>Key differences:</p><ul><li>Strict constraints require immediate enforcement (like traditional uniqueness constraints)</li><li>Loose constraints allow temporary violations that are fixed later</li><li>Many applications can work with loose constraints plus compensating actions (like overbooking + refunds)</li><li>Loose constraints enable better performance and availability</li></ul>;<i>(Ref: Loosely interpreted constraints, p. 526-527)</i>
What is the "trust, but verify" approach to data systems?;<p>Key aspects:</p><ul><li>Systems should not blindly trust that operations work correctly</li><li>Regular auditing and verification of data integrity is necessary</li><li>Systems should be self-validating where possible</li><li>Importance of checking backups and running integrity tests</li><li>Need for cryptographic verification in some cases</li></ul>;<i>(Ref: Trust, but Verify, p. 528-530)</i>
How can event streams improve client-side state management?;<p>Benefits include:</p><ul><li>Enables offline-first applications</li><li>Allows clients to maintain local state</li><li>Supports real-time updates through event subscriptions</li><li>Reduces need for synchronous network requests</li><li>Improves user experience and responsiveness</li></ul>;<i>(Ref: Stateful, offline-capable clients, p. 511-512)</i>
How does the current data revolution compare to the Industrial Revolution?;<p>Key parallels:</p><ul><li>Both brought major technological and economic changes</li><li>Initial lack of safeguards and regulations</li><li>Data is compared to industrial pollution</li><li>Need for protection of individual rights</li><li>Necessity of developing new regulations and ethical frameworks</li></ul>;<i>(Ref: Remembering the Industrial Revolution, p. 541-542)</i>